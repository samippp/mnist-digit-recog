{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf712d6d-c2ee-4807-a690-e21363b3153e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d44b3b11-e51f-4168-b8f7-097d508f71d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z): #relu func for normalization at the hidden layers\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def softmax(Z):\n",
    "    exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "    return exp_Z / exp_Z.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01201907-4ca2-44d0-a943-2cbe1481e78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(input_size, hidden_size1, hidden_size2, output_size):\n",
    "    w1 = np.random.randn(input_size, hidden_size1) * 0.01\n",
    "    b1 = np.zeros((1, hidden_size1))\n",
    "    w2 = np.random.randn(hidden_size1,hidden_size2) * 0.01\n",
    "    b2 = np.zeros((1, hidden_size2))\n",
    "    w3 = np.random.randn(hidden_size2, output_size) * 0.01\n",
    "    b3 = np.zeros((1,output_size))\n",
    "    return w1,b1,w2,b2,w3,b3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd8f189c-7868-4ece-9ae0-daa139ff72f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(x, w1, b1, w2, b2, w3, b3,g1,beta1,g2,beta2):\n",
    "    z1 = np.dot(x, w1) + b1\n",
    "    \n",
    "    epsilon = 1e-5\n",
    "    #batch norm\n",
    "    batch_norm_m1 = np.mean(z1,axis=0)\n",
    "    batch_norm_var1 = np.var(z1,axis=0)\n",
    "\n",
    "    z1_hat = (z1 - batch_norm_m1) / np.sqrt(batch_norm_var1 + epsilon)\n",
    "\n",
    "    y1 = g1 * z1_hat + beta1\n",
    "    a1 = relu(y1)\n",
    "\n",
    "    z2 = np.dot(a1, w2) + b2   \n",
    "\n",
    "    batch_norm_m2 = np.mean(z2, axis=0)\n",
    "    batch_norm_var2 = np.var(z2,axis=0)\n",
    "\n",
    "    z2_hat = (z2 - batch_norm_m2) / np.sqrt(batch_norm_var2 + epsilon)\n",
    "\n",
    "    y2 = g2 * z2_hat + beta2\n",
    "    a2 = relu(y2)\n",
    "\n",
    "    z3 = np.dot(a2, w3) + b3  \n",
    "    a3 = softmax(z3)\n",
    "\n",
    "    batch_norm_cache = y1,y2, z1_hat, z2_hat\n",
    "\n",
    "    return a1, a2, a3, z1, z2, z3, batch_norm_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5150235-560d-4fae-9713-3938e8ab447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv_reLu(y):\n",
    "    return y > 0\n",
    "\n",
    "def back_prop(x,y,a1,a2,a3,w2,w3,z1,z2,g1,beta1,g2,beta2,batch_norm_cache):\n",
    "    y1,y2,z1_hat,z2_hat = batch_norm_cache\n",
    "    m = y.shape[0]\n",
    "    dC3 = a3 - y\n",
    "    dW3 = a2.T.dot(dC3) / m\n",
    "    dB3 = 1 / m * np.sum(dC3, axis=0, keepdims=True)\n",
    "    \n",
    "    d_gamma2 = dC3.dot(w3.T) * deriv_reLu(y2) * z2_hat\n",
    "    d_beta2 = dC3.dot(w3.T) * deriv_reLu(y2)\n",
    "    dC2 = dC3.dot(w3.T) * deriv_reLu(z2)\n",
    "    dW2 = 1 / m * a1.T.dot(dC2)\n",
    "    dB2 = 1 / m * np.sum(dC2, axis=0, keepdims=True)\n",
    "\n",
    "    dC1 = dC2.dot(w2.T) * deriv_reLu(z1)\n",
    "    dW1 = 1 / m * x.T.dot(dC1)\n",
    "    dB1 = 1 / m * np.sum(dC1, axis=0, keepdims=True)\n",
    "    return dW3, dB3, dW2, dB2, dW1, dB1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9fe2541-5bbb-4510-ad99-d1a2910c44f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(w1,b1,w2,b2,w3,b3,dW3, dB3, dW2, dB2, dW1, dB1, learning_rate):\n",
    "    w1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * dB1\n",
    "    w2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * dB2\n",
    "    w3 -= learning_rate * dW3\n",
    "    b3 -= learning_rate * dB3\n",
    "    return (w1,b2,w2,b2,w3,b3)\n",
    "\n",
    "def predict(predictions):\n",
    "    return np.argmax(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c899b5c7-9712-41e7-9b86-cd7781d12c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(Y, A3):#https://www.parasdahal.com/softmax-crossentropy\n",
    "    m = Y.shape[0]\n",
    "    log_likelihood = -np.log(A3[range(m), Y.argmax(axis=1)])\n",
    "    loss = np.sum(log_likelihood) / m\n",
    "    return loss\n",
    "\n",
    "def batch_gradient_descent(x, y, iterations, learning_rate, init):\n",
    "    w1,b1,w2,b2,w3,b3 = init\n",
    "    g1= np.ones((1,128))\n",
    "    beta1 = np.zeros((1,128))\n",
    "    g2 = np.ones((1,64))\n",
    "    beta2 = np.zeros((1,64))\n",
    "    for i in range(iterations):\n",
    "        a1,a2,a3,z1,z2,z3,batch_norm_cache = forward_prop(x,w1, b1, w2, b2, w3, b3, g1,beta1,g2,beta2)\n",
    "        loss = compute_loss(y, a3)\n",
    "        dW3, dB3, dW2, dB2, dW1, dB1 = back_prop(x,y,a1,a2,a3,w2,w3,z1,z2,g1, beta1, g2, beta2, batch_norm_cache)\n",
    "\n",
    "        w1,b2,w2,b2,w3,b3 = update_params(w1,b1,w2,b2,w3,b3,dW3,dB3,dW2,dB2,dW1,dB1, learning_rate)\n",
    "        if( i % 10 == 0):\n",
    "            print(\"Iteration\", i)\n",
    "            print(\"Loss: \",  loss)\n",
    "\n",
    "    return w1,b1,w2,b2,w3,b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f89a984-3ef0-4f7b-bb4b-5c2b8da712ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "back_prop() missing 5 required positional arguments: 'g1', 'beta1', 'g2', 'beta2', and 'batch_norm_cache'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m y_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m10\u001b[39m)[y_test]\n\u001b[0;32m      7\u001b[0m init \u001b[38;5;241m=\u001b[39m init_params(\u001b[38;5;241m784\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m new_params \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_gradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[23], line 16\u001b[0m, in \u001b[0;36mbatch_gradient_descent\u001b[1;34m(x, y, iterations, learning_rate, init)\u001b[0m\n\u001b[0;32m     14\u001b[0m a1,a2,a3,z1,z2,z3,batch_norm_cache \u001b[38;5;241m=\u001b[39m forward_prop(x,w1, b1, w2, b2, w3, b3, g1,beta1,g2,beta2)\n\u001b[0;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m compute_loss(y, a3)\n\u001b[1;32m---> 16\u001b[0m dW3, dB3, dW2, dB2, dW1, dB1 \u001b[38;5;241m=\u001b[39m \u001b[43mback_prop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43ma1\u001b[49m\u001b[43m,\u001b[49m\u001b[43ma2\u001b[49m\u001b[43m,\u001b[49m\u001b[43ma3\u001b[49m\u001b[43m,\u001b[49m\u001b[43mw2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mw3\u001b[49m\u001b[43m,\u001b[49m\u001b[43mz1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mz2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m w1,b2,w2,b2,w3,b3 \u001b[38;5;241m=\u001b[39m update_params(w1,b1,w2,b2,w3,b3,dW3,dB3,dW2,dB2,dW1,dB1, learning_rate)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m( i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "\u001b[1;31mTypeError\u001b[0m: back_prop() missing 5 required positional arguments: 'g1', 'beta1', 'g2', 'beta2', and 'batch_norm_cache'"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], 784) / 255.0  # Flatten and normalize\n",
    "x_test = x_test.reshape(x_test.shape[0], 784) / 255.0\n",
    "y_train = np.eye(10)[y_train]  # One-hot encoding\n",
    "y_test = np.eye(10)[y_test]\n",
    "\n",
    "init = init_params(784, 128, 64, 10)\n",
    "\n",
    "new_params = batch_gradient_descent(x_train,y_train,1,0.6, init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d82b2a-a432-436d-b835-a690f39c855a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37334cb1-e151-4df0-8a23-d9f25055eb08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04de6b88-6c9d-453b-8f69-46e060e5559c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
